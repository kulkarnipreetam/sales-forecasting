{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os\nimport warnings\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom tensorflow.keras import layers, models, optimizers, regularizers\nfrom tensorflow.keras.losses import Huber\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category = UserWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-08-27T22:11:50.919846Z","iopub.execute_input":"2025-08-27T22:11:50.920377Z","iopub.status.idle":"2025-08-27T22:11:50.928406Z","shell.execute_reply.started":"2025-08-27T22:11:50.920339Z","shell.execute_reply":"2025-08-27T22:11:50.926794Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"'''\nLoading the data\n'''\n\nfolder_path = \"/kaggle/input/store-sales-time-series-forecasting/\"\n\nholiday_event_df = pd.read_csv(os.path.join(folder_path,\"holidays_events.csv\"),\n                               dtype={'type': 'category',\n                                      'locale': 'category',\n                                      'locale_name': 'category',\n                                      'description': 'category',\n                                      'transferred': 'bool'},\n                               parse_dates=['date'],\n                               infer_datetime_format=True)\n\nholiday_event_df = holiday_event_df.set_index('date').to_period('D')\n\nholiday_event_df_duplicates = holiday_event_df[holiday_event_df.index.duplicated(keep=False)] #Find duplicate values in holidays\n\nholiday_event_df_without_duplicates = holiday_event_df[~holiday_event_df.index.duplicated(keep='first')] #Handle duplicate values in holidays\n\noil_df = pd.read_csv(os.path.join(folder_path,\"oil.csv\"),\n                     parse_dates=['date'],\n                     infer_datetime_format=True)\n\noil_df = oil_df.set_index('date').to_period('D')\n\noil_df = oil_df.interpolate() #Handle missing values in oil prices\n\noil_df.iloc[0] = oil_df.iloc[1] #Handle missing values in oil prices\n\noil_df.rename(columns={\"dcoilwtico\": \"oil_price\"}, inplace = True)\n\nstores_df = pd.read_csv(os.path.join(folder_path,\"stores.csv\"))\n\ntransaction_df = pd.read_csv(os.path.join(folder_path,\"transactions.csv\"),\n                             parse_dates=['date'],\n                             infer_datetime_format=True)\n\ntransaction_df['date'] = transaction_df['date'].dt.to_period('D')\ntransaction_df = transaction_df.set_index(['date', 'store_nbr']).sort_index()\n\ntrain_df = pd.read_csv(os.path.join(folder_path,\"train.csv\"),\n                                        usecols=['store_nbr', 'family', 'date','sales', 'onpromotion'],\n                                        dtype={'store_nbr': 'category',\n                                               'family': 'category',\n                                               'sales': 'float'},\n                                        parse_dates=['date'],\n                                        infer_datetime_format=True)\n\ntrain_df['date'] = train_df.date.dt.to_period('D')\ntrain_df = train_df.set_index('date').sort_index()\n\ncompetition_test_df = pd.read_csv(os.path.join(folder_path,\"test.csv\"),\n                                  usecols=['id','store_nbr', 'family', 'date', 'onpromotion'],\n                                  dtype={'store_nbr': 'category',\n                                         'family': 'category',\n                                         'onpromotion': 'uint32'},\n                                  parse_dates=['date'],\n                                  infer_datetime_format=True)\n\ncompetition_test_df['date'] = competition_test_df.date.dt.to_period('D')\ncompetition_test_df = competition_test_df.set_index('date').sort_index()","metadata":{"execution":{"iopub.status.busy":"2025-08-27T22:11:50.930907Z","iopub.execute_input":"2025-08-27T22:11:50.931447Z","iopub.status.idle":"2025-08-27T22:12:04.739561Z","shell.execute_reply.started":"2025-08-27T22:11:50.931391Z","shell.execute_reply":"2025-08-27T22:12:04.737838Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"'''\nBuilding a neural network to predict sales by store and family.\n'''\n\ndef rmsle(y_true, y_pred):\n    # Convert to numpy arrays\n    y_true = np.array(y_true).flatten()\n    y_pred = np.array(y_pred).flatten()\n    \n    # Clip negative values to 0\n    y_true = np.clip(y_true, 0, None)\n    y_pred = np.clip(y_pred, 0, None)\n    \n    # Compute RMSLE\n    return np.sqrt(np.mean((np.log1p(y_pred) - np.log1p(y_true))**2))\n\ndef rmsle_keras(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred, 0, np.inf)\n    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(y_pred) - tf.math.log1p(y_true))))\n    \nX = train_df.loc['2016':'2017'].copy()\n\nX['day'] = X.index.day\nX['week'] = X.index.dayofweek\n\nX = X.join(oil_df, on='date')\nX['oil_price'] = X['oil_price'].interpolate()\n\nX['NewYear'] = (X.index.dayofyear == 1)\nX['holiday'] = X.index.to_series().isin(holiday_event_df.index)\n\ny = X[['sales']].copy()\n\nX_competition = competition_test_df.loc['2016':'2017'].copy()\n\nX_competition['day'] = X_competition.index.day\nX_competition['week'] = X_competition.index.dayofweek\n\nX_competition = X_competition.join(oil_df, on='date')\nX_competition['oil_price'] = X_competition['oil_price'].interpolate()\n\nX_competition['NewYear'] = (X_competition.index.dayofyear == 1)\nX_competition['holiday'] = X_competition.index.to_series().isin(holiday_event_df.index)\n\ny_train, y_test = y[:\"2017-06-01\"], y[\"2017-06-02\":]\nX_train, X_test = X.loc[:\"2017-06-01\"], X.loc[\"2017-06-02\":]\n\nscaler = StandardScaler()\n\ncolumns_to_be_scaled = ['onpromotion', 'oil_price', 'day']\n\nscaled_train_data = scaler.fit_transform(X_train.loc[:,columns_to_be_scaled])\n\nscaled_train_data_df = pd.DataFrame(scaled_train_data, columns=[i + \"_scaled\" for i in columns_to_be_scaled], index=X_train.index)\n\nX_train = pd.concat([X_train, scaled_train_data_df], axis=1)\n\nscaled_test_data = scaler.transform(X_test.loc[:,columns_to_be_scaled])\n\nscaled_test_data_df = pd.DataFrame(scaled_test_data, columns=[i + \"_scaled\" for i in columns_to_be_scaled], index=X_test.index)\n\nX_test = pd.concat([X_test, scaled_test_data_df], axis=1)\n\nscaled_competition_data = scaler.transform(X_competition.loc[:,columns_to_be_scaled])\n\nscaled_competition_data_df = pd.DataFrame(scaled_competition_data, columns=[i + \"_scaled\" for i in columns_to_be_scaled], index=X_competition.index)\n\nX_competition = pd.concat([X_competition, scaled_competition_data_df], axis=1)\n\ncolumns_to_be_encoded = ['week', 'family', 'store_nbr']\n\nfor column in columns_to_be_encoded:\n    \n    onehot_encoder = OneHotEncoder()\n    \n    encoded_train_column = onehot_encoder.fit_transform(X_train[column].values.reshape(-1, 1))\n\n    encoded_train_column_df = pd.DataFrame(encoded_train_column.toarray(), columns=onehot_encoder.get_feature_names_out([column]), index=X_train.index)\n    \n    X_train = pd.concat([X_train, encoded_train_column_df], axis=1)\n    \n    encoded_test_column = onehot_encoder.transform(X_test[column].values.reshape(-1, 1))\n\n    encoded_test_column_df = pd.DataFrame(encoded_test_column.toarray(), columns=onehot_encoder.get_feature_names_out([column]), index=X_test.index)\n    \n    X_test = pd.concat([X_test, encoded_test_column_df], axis=1)\n    \n    encoded_competition_column = onehot_encoder.transform(X_competition[column].values.reshape(-1, 1))\n\n    encoded_competition_column_df = pd.DataFrame(encoded_competition_column.toarray(), columns=onehot_encoder.get_feature_names_out([column]), index=X_competition.index)\n    \n    X_competition = pd.concat([X_competition, encoded_competition_column_df], axis=1)\n\nX_train.drop(columns=columns_to_be_scaled + ['sales'] + columns_to_be_encoded, inplace = True)\n\nX_train[['NewYear','holiday']] = X_train[['NewYear','holiday']].astype(int)\n\nX_test.drop(columns=columns_to_be_scaled + ['sales'] + columns_to_be_encoded, inplace = True)\n\nX_test[['NewYear','holiday']] = X_test[['NewYear','holiday']].astype(int)\n\nX_competition.drop(columns=columns_to_be_scaled + columns_to_be_encoded, inplace = True)\n\nX_competition[['NewYear','holiday']] = X_competition[['NewYear','holiday']].astype(int)\n\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Dense(32, activation='relu'))\n\nmodel.add(layers.Dense(1, activation='linear'))\n\noptimizer = optimizers.Adam(learning_rate=0.001)\n\nhuber_loss = Huber(delta=1.0)\n\nmodel.compile(optimizer=optimizer, loss=huber_loss, metrics=[rmsle_keras])\n\nmodel_params = {'epochs' : 20, 'batch_size' : 64, 'validation_split' : 0.1}\n\nmodel.fit(X_train,y_train, **model_params)\n\ny_pred = pd.DataFrame(model.predict(X_test).flatten(), index=X_test.index, columns=['sales'])\n\nprint(\"\\nTest RMSLE of the neural network model: {}\\n\".format(rmsle(y_test, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2025-08-27T22:12:04.742179Z","iopub.execute_input":"2025-08-27T22:12:04.742735Z","iopub.status.idle":"2025-08-27T22:42:20.777882Z","shell.execute_reply.started":"2025-08-27T22:12:04.742665Z","shell.execute_reply":"2025-08-27T22:42:20.776229Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 7ms/step - loss: 200.6187 - rmsle_keras: 1.2428 - val_loss: 104.6017 - val_rmsle_keras: 0.7306\nEpoch 2/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 7ms/step - loss: 114.7272 - rmsle_keras: 0.8776 - val_loss: 100.8473 - val_rmsle_keras: 0.7207\nEpoch 3/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 7ms/step - loss: 106.8144 - rmsle_keras: 0.8324 - val_loss: 98.3412 - val_rmsle_keras: 0.7044\nEpoch 4/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7ms/step - loss: 103.0149 - rmsle_keras: 0.8099 - val_loss: 100.9040 - val_rmsle_keras: 0.7168\nEpoch 5/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - loss: 100.2324 - rmsle_keras: 0.7982 - val_loss: 100.4377 - val_rmsle_keras: 0.7183\nEpoch 6/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 7ms/step - loss: 99.3575 - rmsle_keras: 0.7887 - val_loss: 89.6191 - val_rmsle_keras: 0.6868\nEpoch 7/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - loss: 96.8264 - rmsle_keras: 0.7760 - val_loss: 97.0016 - val_rmsle_keras: 0.6874\nEpoch 8/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 7ms/step - loss: 96.5307 - rmsle_keras: 0.7746 - val_loss: 106.7554 - val_rmsle_keras: 0.6915\nEpoch 9/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7ms/step - loss: 95.1878 - rmsle_keras: 0.7674 - val_loss: 97.5536 - val_rmsle_keras: 0.6914\nEpoch 10/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7ms/step - loss: 94.1898 - rmsle_keras: 0.7677 - val_loss: 93.9182 - val_rmsle_keras: 0.7122\nEpoch 11/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 7ms/step - loss: 94.3784 - rmsle_keras: 0.7679 - val_loss: 98.9310 - val_rmsle_keras: 0.6955\nEpoch 12/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 7ms/step - loss: 93.7138 - rmsle_keras: 0.7613 - val_loss: 104.7606 - val_rmsle_keras: 0.7011\nEpoch 13/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - loss: 92.6554 - rmsle_keras: 0.7546 - val_loss: 93.6548 - val_rmsle_keras: 0.6903\nEpoch 14/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7ms/step - loss: 91.9644 - rmsle_keras: 0.7569 - val_loss: 100.5069 - val_rmsle_keras: 0.7028\nEpoch 15/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 7ms/step - loss: 91.4146 - rmsle_keras: 0.7578 - val_loss: 96.0644 - val_rmsle_keras: 0.7043\nEpoch 16/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7ms/step - loss: 90.7635 - rmsle_keras: 0.7501 - val_loss: 97.4598 - val_rmsle_keras: 0.7171\nEpoch 17/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 7ms/step - loss: 90.6919 - rmsle_keras: 0.7488 - val_loss: 96.1621 - val_rmsle_keras: 0.6915\nEpoch 18/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 7ms/step - loss: 90.2859 - rmsle_keras: 0.7525 - val_loss: 96.5966 - val_rmsle_keras: 0.6815\nEpoch 19/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 7ms/step - loss: 90.2929 - rmsle_keras: 0.7472 - val_loss: 94.0170 - val_rmsle_keras: 0.6890\nEpoch 20/20\n\u001b[1m12956/12956\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 7ms/step - loss: 89.8265 - rmsle_keras: 0.7441 - val_loss: 98.7427 - val_rmsle_keras: 0.7026\n\u001b[1m4177/4177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step\n\nTest RMSLE of the neural network model: 0.8712321904958976\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"'''\nPredicting sales for the competition dataset\n'''\nX_competition = X_competition[X_train.columns]\n\ny_submit = competition_test_df[['id']].copy()\ny_submit['sales'] = model.predict(X_competition).flatten()\ny_submit['sales'] = np.clip(y_submit['sales'], 0, None)\ny_submit.to_csv('submission.csv', index=False)\n\nprint(\"\\nBelow are the predictions for the competition data:\")\nprint(y_submit)","metadata":{"execution":{"iopub.status.busy":"2025-08-27T22:42:20.779789Z","iopub.execute_input":"2025-08-27T22:42:20.780277Z","iopub.status.idle":"2025-08-27T22:42:22.934847Z","shell.execute_reply.started":"2025-08-27T22:42:20.780224Z","shell.execute_reply":"2025-08-27T22:42:22.933602Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n\nBelow are the predictions for the competition data:\n                 id        sales\ndate                            \n2017-08-16  3000888     1.045129\n2017-08-16  3000889     0.217918\n2017-08-16  3000890     0.997175\n2017-08-16  3000891  2186.218506\n2017-08-16  3000892     0.264605\n...             ...          ...\n2017-08-31  3029395   333.258118\n2017-08-31  3029396    70.656784\n2017-08-31  3029397  1042.343140\n2017-08-31  3029398     0.888752\n2017-08-31  3029399    11.562046\n\n[28512 rows x 2 columns]\n","output_type":"stream"}],"execution_count":16}]}